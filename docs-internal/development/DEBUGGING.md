# Debugging

## Logging filter

Logging is implemented using Tracing's [`EnvFilter`](https://docs.rs/tracing-subscriber/latest/tracing_subscriber/filter/struct.EnvFilter.html).

This filter can be configured by setting the `RUST_LOG` environment variable.

To enable all debug logging, set the env var: `RUST_LOG=debug`

For example, to enable debug logging for the `cluster` crate, set the env var: `RUST_LOG=cluster=debug`

Read more on how to use the filter [here](https://docs.rs/tracing-subscriber/latest/tracing_subscriber/filter/struct.EnvFilter.html).

# Tracing instruments

Every log has a stack trace associated with it. This is not a native Rust backtrace; instead this is a backtrace to any function with `#[tracing::instrument]` defined on it.

This allows us to understand where logs came from without incurring the overhead of Rust's native backtraces.

# Tokio console

Tokio Console can be enabled by setting the env var `TOKIO_CONSOLE_ENABLE=1` and visiting 127.0.0.1:6669. This address can be overridden with `TOKIO_CONSOLE_BIND`.

# Logging properties

- `RUST_LOG` (see [`EnvFilter`](https://docs.rs/tracing-subscriber/latest/tracing_subscriber/filter/struct.EnvFilter.html))
- `TOKIO_CONSOLE_ENABLE`
- `TOKIO_CONSOLE_BIND`
- `RUST_LOG_SPAN_NAME`
- `RUST_LOG_SPAN_PATH`
- `RUST_LOG_TARGET`
- `RUST_LOG_LOCATION`
- `RUST_LOG_MODULE_PATH`
- `RUST_LOG_ANSI_COLOR`

## Reading logs

Rivet uses structured logging, which can be verbose and hard to read at times.

Here is an example log:

```json
{
	"timestamp": "2023-11-14T20:24:34.038004Z",
	"level": "INFO",
	"fields": {
		"message": "operation call",
		"body": "Request { user_ids: [Uuid(6bda49ad-355c-4f0b-b76f-cb773f4ba9df)] }"
	},
	"target": "rivet_operation",
	"spans": [
		{
			"method": "GET",
			"ray_id": "f14d9ff6-06bf-42dc-8939-8e228c4baa0f",
			"uri": "/cloud/games?watch_index=1699993473573",
			"name": "http request"
		},
		{ "operation": "user-get", "name": "call" }
	]
}
```

- `target` is the name of the span (usually the function name) that made the log
- `fields` are the relevant information to the given function call
- `spans` are an abbreviated stack trace providing information about where the log came from
  - `spans[0].ray_id` is important, see below

### Parsing & formatting logs

Loki's [`json`](https://grafana.com/docs/loki/latest/query/log_queries/#parser-expression) parser and
[`line_format`](https://grafana.com/docs/loki/latest/query/log_queries/#line-format-expression) expression are
helpful for parsing structured logs.

## Rays & requests

Any code that runs in Rivet has an associated ray ID & request ID. These IDs are UUIDs generated by Rivet, not
by an external provider.

A ray correlates a series of actions that started from a single event (such as an HTTP request or CRON job).

A request represents what happens in a single `OperationContext` (e.g. operation call, consuming a message,
HTTP request).

There are multiple requests for a single ray.

### Querying by ray

When running in to any issue, the first thing you want to do is find the ray ID that this issue came from.

Use that ray ID and query Loki with the following:

```
{name=~"rivet-.*"} |= "ead90acf-8a95-43c8-9c8b-8671fb373cca"
```

This will give you all logs associated with the problem. From here, you can either narrow down the service or
filter by errors.

### Helpful log messages

The following log messages are commonly queried against in Loki to narrow down events:

- `operation call` provides the request for an operation
- `operation response` provides the response for an operation
- `publish message` provides the source & body of a published message
- `received message` is logged when workers consume a message
- `worker success` is logged when a worker finishes

## Testing

Rivet is extensively tested, which makes it easy to narrow down the cause of an error.

### Reading test logs

Use `cat` to read the logs at the given path. If wish to read the logs in real time, use
`tail -f <path to logs>`.

Test logs are also available in Loki.

## Monitoring performance & errors

Rivet exposes extensive Prometheus metrics on our internal services. Look for the following Grafana
dashboards:

- `Chirp / API` for performance & errors on API services
- `Chirp / Operation` for performance, errors, & consumer monitoring on operations & consumers
- `Chirp / Perf Spans` for performance of narrow portions of services
- `Rivet / SQL` for monitoring SQL queries & pools

## Alerting

Rivet uses Alert Manager extensively for catching errors before they happen & quickly narrowing down the
source of errors. Alerts can be pushed to Slack if the `alertmanager/slack/url` and
`alertmanager/slack/channel` secrets are provided. See _infra/tf/k8s_infra/prometheus.tf_.
